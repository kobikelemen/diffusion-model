{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "beta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "device = 'cuda:0'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dat = datasets.MNIST(\n",
    "    data_path, train=True, download=True, transform=transform\n",
    ")\n",
    "test_dat = datasets.MNIST(data_path, train=False, transform=transform)\n",
    "\n",
    "loader_train = DataLoader(train_dat, batch_size, shuffle=True, generator=torch.Generator(device=device))\n",
    "loader_test = DataLoader(test_dat, batch_size, shuffle=False, generator=torch.Generator(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_time_embedding(num_steps, emb_dim):\n",
    "    embedding_mat = torch.empty((num_steps, emb_dim))\n",
    "    for step in range(num_steps):\n",
    "        embedding = torch.tensor([1 / 10000 ** (2*i / emb_dim) for i in range(emb_dim)])\n",
    "        if step % 2 == 0:\n",
    "            embedding = (embedding * step).sin()\n",
    "        else:\n",
    "            embedding = (embedding * step).cos()\n",
    "        embedding_mat[step] = embedding\n",
    "    time_embed = nn.Embedding(num_steps, emb_dim)\n",
    "    time_embed.weight.data = embedding_mat\n",
    "    time_embed.requires_grad_(False)\n",
    "    return time_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_time_embedding_mlp(in_d, out_d):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_d, out_d),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(out_d, out_d)\n",
    "    )\n",
    "        \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, normalize=True):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln = nn.LayerNorm(shape)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ln(x) if self.normalize else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\" Using a UNet architecture as noise function approximator \"\"\"\n",
    "    def __init__(self, num_steps=1000, emb_dim=100):\n",
    "        super(UNet, self).__init__()\n",
    "        self.time_embed = make_time_embedding(num_steps, emb_dim)\n",
    "        self.te1 = make_time_embedding_mlp(emb_dim, 1)\n",
    "        self.b1 = nn.Sequential(\n",
    "            Block((1, 28, 28), 1, 10),\n",
    "            Block((10, 28, 28), 10, 10),\n",
    "            Block((10, 28, 28), 10, 10)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
    "\n",
    "        self.te2 = make_time_embedding_mlp(emb_dim, 10)\n",
    "        self.b2 = nn.Sequential(\n",
    "            Block((10, 14, 14), 10, 20),\n",
    "            Block((20, 14, 14), 20, 20),\n",
    "            Block((20, 14, 14), 20, 20)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
    "\n",
    "        self.te3 = make_time_embedding_mlp(emb_dim, 20)\n",
    "        self.b3 = nn.Sequential(\n",
    "            Block((20, 7, 7), 20, 40),\n",
    "            Block((40, 7, 7), 40, 40),\n",
    "            Block((40, 7, 7), 40, 40)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(40, 40, 4, 2, 1)\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.te_mid = make_time_embedding_mlp(emb_dim, 40)\n",
    "        self.b_mid = nn.Sequential(\n",
    "            Block((40, 3, 3), 40, 20),\n",
    "            Block((20, 3, 3), 20, 20),\n",
    "            Block((20, 3, 3), 20, 40)\n",
    "        )\n",
    "\n",
    "        # Second half\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(40, 40, 4, 2, 1),\n",
    "            nn.SiLU(),\n",
    "            nn.ConvTranspose2d(40, 40, 2, 1)\n",
    "        )\n",
    "\n",
    "        self.te4 = make_time_embedding_mlp(emb_dim, 80)\n",
    "        self.b4 = nn.Sequential(\n",
    "            Block((80, 7, 7), 80, 40),\n",
    "            Block((40, 7, 7), 40, 20),\n",
    "            Block((20, 7, 7), 20, 20)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
    "        self.te5 = make_time_embedding_mlp(emb_dim, 40)\n",
    "        self.b5 = nn.Sequential(\n",
    "            Block((40, 14, 14), 40, 20),\n",
    "            Block((20, 14, 14), 20, 10),\n",
    "            Block((10, 14, 14), 10, 10)\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
    "        self.te_out = make_time_embedding_mlp(emb_dim, 20)\n",
    "        self.b_out = nn.Sequential(\n",
    "            Block((20, 28, 28), 20, 10),\n",
    "            Block((10, 28, 28), 10, 10),\n",
    "            Block((10, 28, 28), 10, 10, normalize=False)\n",
    "        )\n",
    "\n",
    "        self.conv_out = nn.Conv2d(10, 1, 3, 1, 1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # x is (N, 2, 28, 28) (image with positional embedding stacked on channel dimension)\n",
    "        t = self.time_embed(t)\n",
    "        n = len(x)\n",
    "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (N, 10, 28, 28)\n",
    "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (N, 20, 14, 14)\n",
    "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (N, 40, 7, 7)\n",
    "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (N, 40, 3, 3)\n",
    "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (N, 80, 7, 7)\n",
    "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (N, 20, 7, 7)\n",
    "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (N, 40, 14, 14)\n",
    "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (N, 10, 14, 14)\n",
    "        out = torch.cat((out1, self.up3(out5)), dim=1)  # (N, 20, 28, 28)\n",
    "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (N, 1, 28, 28)\n",
    "        out = self.conv_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_xt(epsilon, beta, t, x0):\n",
    "    \"\"\" Note: this assumes beta doesn't vary with time step \"\"\"\n",
    "    at = (1 - beta) ** t\n",
    "    # print(f'at: {at.shape} x0: {x0.shape} epsilon: {epsilon.shape}')\n",
    "    return torch.sqrt(at.view(batch_size,1,1,1)) * x0 + torch.sqrt(1 - at.view(batch_size,1,1,1)) * epsilon\n",
    "\n",
    "def sample_t(batch_size, timesteps):\n",
    "    return torch.randint(1, timesteps+1, (batch_size,))\n",
    "\n",
    "def sample_epsilon(batch_size, img_h, img_w):\n",
    "    return torch.normal(torch.zeros((batch_size, 1, img_h, img_w)), torch.ones((batch_size, 1, img_h, img_w)))\n",
    "\n",
    "def calc_loss(epsilon, epsilon_pred):\n",
    "    return torch.linalg.vector_norm(epsilon - epsilon_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "img_w = 28\n",
    "img_h = 28\n",
    "\n",
    "torch.set_default_device(device)\n",
    "model = UNet(num_steps=timesteps).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters is: {}\".format(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss, test_loss = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_train_loss = 0\n",
    "    epoch_test_loss = 0\n",
    "    if epoch > 1:\n",
    "        break \n",
    "\n",
    "    with tqdm.tqdm(loader_train, unit=\"batch\") as tepoch:\n",
    "        for batch_idx, (data, _) in enumerate(tepoch):\n",
    "            data = data.to(device)\n",
    "            epsilon = sample_epsilon(batch_size, img_h, img_w)\n",
    "            t = sample_t(batch_size, timesteps)\n",
    "            xt = calc_xt(epsilon, beta, t, data)\n",
    "            epsilon_pred = model(xt, t)\n",
    "            loss = calc_loss(epsilon, epsilon_pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_train_loss += loss.item()\n",
    "            if batch_idx % 20 == 0:\n",
    "                tepoch.set_description(f\"Train Epoch {epoch}\")\n",
    "                tepoch.set_postfix(loss=epoch_train_loss/((batch_idx+1) * batch_size))\n",
    "        print(f'Train Loss: {epoch_train_loss / (len(tepoch) * batch_size)}')\n",
    "        train_loss.append(epoch_train_loss / (len(tepoch) * batch_size))\n",
    "\n",
    "        with tqdm.tqdm(loader_test, unit=\"batch\") as tepoch:\n",
    "            for batch_idx, (data, _) in enumerate(tepoch):\n",
    "                data = data.to(device)\n",
    "                with torch.no_grad():\n",
    "                    epsilon = sample_epsilon(batch_size, img_h, img_w)\n",
    "                    t = sample_t(batch_size, timesteps)\n",
    "                    xt = calc_xt(epsilon, beta, t, data)\n",
    "                    epsilon_pred = model(xt, t)\n",
    "                    loss = calc_loss(epsilon, epsilon_pred)\n",
    "                    epoch_test_loss += loss.item()\n",
    "            print(f'Test Loss: {epoch_test_loss / (len(tepoch) * batch_size)}')\n",
    "            test_loss.append(epoch_test_loss / (len(tepoch) * batch_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor([3,3,3])\n",
    "x2 = torch.tensor([2,2,2])\n",
    "print((x1+1) ** x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor([1,2,3,4])\n",
    "x2 = torch.tensor([[3,3],[3,3],[3,3],[3,3]])\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "x3 = x1.view((4,1)) * x2\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kk2720/dl/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_vars.py\", line 624, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "NameError: name 'tensor' is not defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m     dist\u001b[38;5;241m.\u001b[39minit_process_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m, rank\u001b[38;5;241m=\u001b[39mrank, world_size\u001b[38;5;241m=\u001b[39mworld_size)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/kk2720/dl/diffusion-model/model/cifar_simple_diffusion8.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# model = UNet(1000,100)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# setup(0,1)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     model \u001b[38;5;241m=\u001b[39m AttentionUNet(\u001b[38;5;241m32\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m     dist\u001b[38;5;241m.\u001b[39minit_process_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnccl\u001b[39m\u001b[38;5;124m\"\u001b[39m, rank\u001b[38;5;241m=\u001b[39mrank, world_size\u001b[38;5;241m=\u001b[39mworld_size)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/kk2720/dl/diffusion-model/model/cifar_simple_diffusion8.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# model = UNet(1000,100)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# setup(0,1)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     model \u001b[38;5;241m=\u001b[39m AttentionUNet(\u001b[38;5;241m32\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/dl/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/dl/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from unet.unet import OneResUNet, AttentionUNet\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "# from unet import UNet\n",
    "\n",
    "def calc_beta(min_b, max_b, t, max_t):\n",
    "    return min_b + (t/max_t) * (max_b - min_b) \n",
    "\n",
    "def calc_lists(min_b, max_b, num_steps):\n",
    "    at_hat_ = 1\n",
    "    beta_list_, at_list_, at_hat_list_ = [], [], []\n",
    "    for t in range(timesteps):\n",
    "        b = calc_beta(min_b, max_b, t, num_steps)\n",
    "        beta_list_.append(b)\n",
    "        at_list_.append(1-b)\n",
    "        at_hat_ *= (1-b)\n",
    "        at_hat_list_.append(at_hat_)\n",
    "    return torch.tensor(beta_list_).to(device), torch.tensor(at_list_).to(device), torch.tensor(at_hat_list_).to(device)\n",
    "\n",
    "device = 'cuda:0'\n",
    "data_path = './data'\n",
    "batch_size = 8\n",
    "timesteps = 1000\n",
    "# beta = 0.01\n",
    "min_beta = 10**-4\n",
    "max_beta = 0.02\n",
    "# min_beta = 0.01\n",
    "# max_beta = 0.01\n",
    "beta_list, at_list, at_hat_list = calc_lists(min_beta, max_beta, timesteps)\n",
    "\n",
    "def denorm(x, channels=None, w=None ,h=None, resize = False):\n",
    "    mean = torch.Tensor([0.4914])\n",
    "    std = torch.Tensor([0.247])\n",
    "    unnormalize = transforms.Normalize((-mean / std).tolist(), (1.0 / std).tolist())\n",
    "    x = unnormalize(x)\n",
    "    if resize:\n",
    "        if channels is None or w is None or h is None:\n",
    "            print('Number of channels, width and height must be provided for resize.')\n",
    "        x = x.view(x.size(0), channels, w, h)\n",
    "    return x\n",
    "\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "\n",
    "def sample_diffusion_model(model, timesteps, batch_size, xt=None):\n",
    "    if xt == None:\n",
    "        xt = torch.normal(torch.zeros((batch_size,3,32,32)), torch.ones((batch_size,3,32,32))).to(0)\n",
    "    for t in range(timesteps,0,-1):\n",
    "        \n",
    "        cur_beta = calc_beta(min_beta, max_beta, t, timesteps)\n",
    "        sigma_t = sqrt(cur_beta)\n",
    "        at = 1 - cur_beta\n",
    "        at_hat = 1\n",
    "        for s in range(1,t+1):\n",
    "            at_hat *= (1 - calc_beta(min_beta, max_beta, s, timesteps))\n",
    "\n",
    "        z = torch.normal(torch.zeros((batch_size,3,32,32)), torch.ones((batch_size,3,32,32))).to(0)\n",
    "        if t == 1:\n",
    "            z = torch.zeros((batch_size,3,32,32)).to(0)\n",
    "        epsilon_pred = model(xt,torch.full((batch_size,),t))\n",
    "        xt = (xt - epsilon_pred*(1-at)/sqrt(1-at_hat)) / sqrt(at) + sigma_t*z\n",
    "        if t%100 == 0 or t == 1:\n",
    "            xt_ = xt\n",
    "            for b in range(batch_size):\n",
    "                min_val = xt_[b].min()\n",
    "                max_val = xt_[b].max()\n",
    "                xt_[b] = (xt_[b] - min_val) / (max_val - min_val)\n",
    "            xt_ = make_grid(xt_, nrow=8, padding=2, normalize=False,\n",
    "                                value_range=None, scale_each=False, pad_value=0)\n",
    "            plt.figure()\n",
    "            show(xt_)\n",
    "    return xt\n",
    "\n",
    "\n",
    "def encode_img(x0, timesteps, batch_size):\n",
    "    # at_hat = (1 - beta) ** timesteps\n",
    "    at_hat = 1\n",
    "    for s in range(1,timesteps+1):\n",
    "        at_hat *= (1 - calc_beta(min_beta, max_beta, s, timesteps))\n",
    "    mean = sqrt(at_hat) * x0 \n",
    "    std = (1 - at_hat) * torch.ones((batch_size,3,32,32))\n",
    "    print(f'std: {std.shape} mean: {mean.shape}')\n",
    "    xt = torch.normal(mean, std)\n",
    "    return xt\n",
    "\n",
    "def scale_output_for_plotting(xt):\n",
    "    res = (xt / 2) + 0.5\n",
    "    res = torch.clamp(res,min=0,max=1)    \n",
    "    return res\n",
    "\n",
    "\n",
    "def sample_simple_diffusion_model(model):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x - 0.5) * 2)])\n",
    "    test_dat = datasets.MNIST(data_path, train=False, transform=transform)\n",
    "    loader_test = DataLoader(test_dat, batch_size, shuffle=False)\n",
    "    sample_inputs, _ = next(iter(loader_test))\n",
    "    fixed_input = sample_inputs[0:32, :, :, :]\n",
    "    img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                    value_range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure()\n",
    "    show(img)\n",
    "    plt.savefig('/home/kk2720/dl/diffusion-model/plots/reference_mnist.jpeg')\n",
    "    with torch.no_grad():\n",
    "        # xt = encode_img(sample_inputs, timesteps, batch_size)\n",
    "        \n",
    "        recon_batch = sample_diffusion_model(model, timesteps, batch_size, None)\n",
    "        recon_batch = recon_batch.cpu()\n",
    "\n",
    "# def load_ddp_model(path, model):\n",
    "#     model_dict = OrderedDict()\n",
    "#     pattern = re.compile('module.')\n",
    "#     for k,v in state_dict.items():\n",
    "#         if re.search(\"module\", k):\n",
    "#             model_dict[re.sub(pattern, '', k)] = v\n",
    "#         else:\n",
    "#             model_dict = state_dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12357'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = '/home/kk2720/dl/diffusion-model/model/cifar_simple_diffusion8.pt'\n",
    "    # model = UNet(1000,100)\n",
    "    # setup(0,1)\n",
    "    model = AttentionUNet(32, channels=3).to(0)\n",
    "    model = DDP(model, device_ids=[0], output_device=0, find_unused_parameters=False)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    sample_simple_diffusion_model(model)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cifar-10 visualization \"\"\"\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0'\n",
    "data_path = './data'\n",
    "batch_size = 8\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x - 0.5) * 2)])\n",
    "test_dat = datasets.CIFAR10(data_path, train=False, transform=transform, download=True)\n",
    "loader_test = DataLoader(test_dat, batch_size, shuffle=False)\n",
    "sample_inputs, _ = next(iter(loader_test))\n",
    "fixed_input = sample_inputs[0:32, :, :, :]\n",
    "# print(fixed_input)\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                value_range=None, scale_each=False, pad_value=0)\n",
    "plt.figure()\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.ones((8,2))\n",
    "y = torch.randn_like(x)\n",
    "print(f'y: {y}')\n",
    "z = torch.normal(torch.zeros((8,2)), torch.ones((8,2)))\n",
    "print(f'z: {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
